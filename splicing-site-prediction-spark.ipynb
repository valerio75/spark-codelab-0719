{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Splicing Site Prediction using Apache Spark\n",
    "\n",
    "## Notebook Info\n",
    "This notebook shows the use of Apache Spark's MLLib to address the problem of Splicing Site Prediction.\n",
    "## Author Info\n",
    "Valerio Morfino [Linkedin page](https://www.linkedin.com/in/valerio-morfino/)\n",
    "\n",
    "## Notebook Prerequisites\n",
    "Prerequisites: \n",
    "- Python 3.5+\n",
    "- Apache Spark 2.1+\n",
    "- Jupyter Notebook \n",
    "\n",
    "In order to run this snotebook you need the following python libraries:\n",
    "- findspark  (pip3 install findspark)\n",
    "- tweepy\t (pip3 install tweepy)\n",
    "- matplotlib (pip3 install matplotlib)\n",
    "- seaborn    (pip3 install seaborn)\n",
    "\n",
    "## How to run the notebook\n",
    "Set the constant: SPARK_HOME\n",
    "Make sure the dataset folder is in the same directory as the notebook.\n",
    "Enjoy the notebook!\n",
    "\n",
    "## Dataset Info\n",
    "The dataset used is IPDATA (Irvine Primate splice-junction data set). It is a data set of human splice sites, and it consists of 767 donor splice sites, 765 acceptor splice sites, and 1654 false splice sites.\n",
    "\n",
    "\n",
    "## More Info\n",
    "For more info about Apache Spark and MLLib please visit:\n",
    "- [Apache Spark Home Page](https://spark.apache.org/)\n",
    "- [Apache Spark Machine Learning Library (MLlib) Guide](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Spark Home\n",
    "SPARK_HOME = \"/home/osboxes/spark-2.2.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# In this section are defined some function used later in the code\n",
    "#\n",
    "\n",
    "#Custom function to encode nucleotides according to Brain format \n",
    "def encodeDNASeq(seq, encoding='OneHot'):\n",
    "    \"\"\"Encode nucleotides from character to double or OneHot encoding.\n",
    "    Using OneHot nucleotides are encoded as:\n",
    "    A->1000; C->0100; G->0010; T->0001; other->0000\n",
    "    Using Index as: A->1.0; C->2.0; G->3.0; t->4.0; other->0.0\n",
    "    @param: seq A string containing a sequence of nucleotides \n",
    "    @param: encoding_type output encodig: OneHot or Index\n",
    "    \"\"\"    \n",
    "    if encoding==\"Index\":\n",
    "        mymap = {'A':1.0, 'C':2.0, 'G':3.0, 'T':4.0}\n",
    "\n",
    "    else:\n",
    "        mymap ={'A':SparseVector(4, [0], [1]), \n",
    "                'C':SparseVector(4, [1], [1]), \n",
    "                'G':SparseVector(4, [2], [1]), \n",
    "                'T':SparseVector(4, [3], [1])}    \n",
    "    \n",
    "    indexed_seq=list()\n",
    "    #Verificare se si può sostituire con qualcosa di parallelizzabile\n",
    "    #Mettere quì la selezione della finestra di osservazione\n",
    "    for n in seq:\n",
    "       indexed_seq.append(mymap.get(n) if n in mymap else SparseVector(4, [0], [0]))\n",
    "    return indexed_seq   \n",
    "\n",
    "#Split each line in single features\n",
    "#encode each nucleotide using function encodeDNASeq\n",
    "def load_dna_dataset(file_name,label_value, nrows=0, encoding='OneHot'):\n",
    "    \"Read Input Dataset contained in file_name. Data are labelled with value specified in label_value parameter\"\n",
    "    rdd = sc.textFile(file_name).flatMap(lambda line: [list(line)]).map(lambda s: encodeDNASeq(s,encoding)) \n",
    "        \n",
    "    #Insert Label Column and convert Rdd into Dataframe in order to apply ML Algorithm\n",
    "    return rdd.toDF().withColumn(\"label\",lit(label_value))\n",
    "\n",
    "\n",
    "\n",
    "#Plot Confusion Matrix and print all related indicators\n",
    "def confusion_matrix(predictions_and_labels, print_heat_cm=True, print_Pandas_cm=False, print_summary=False ):\n",
    "    \"Print a summary of prediction via Confusion Matrix and other indicators. Return Confusion Matrix as array\"\n",
    "    import seaborn as sn\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pandas_ml import ConfusionMatrix\n",
    "    \n",
    "    df=predictions_and_labels.toPandas()\n",
    "    cm = ConfusionMatrix(df['label'],df['prediction'])\n",
    "    \n",
    "    if print_Pandas_cm:\n",
    "        cm.plot()\n",
    "\n",
    "    if print_heat_cm:\n",
    "        df_cm = pd.DataFrame(cm.to_array(), index = [0,1], columns = [0,1],)\n",
    "        plt.figure(figsize = (10,7))\n",
    "        ax=sn.heatmap(df_cm, annot=True,fmt=\"d\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "    \n",
    "    if print_summary:\n",
    "        cm.print_stats()\n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the Spark Home\n",
    "import findspark\n",
    "findspark.init(SPARK_HOME)\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import time\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#Inizialize Spark Context\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"splicing_site_prediction\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Training Set\n",
    "#The input dataset is composed of 4 files: Positive instances and negative instances (Training and Test)\n",
    "#Each data row cointains a string of nucleotides: A G C T.\n",
    "#We will load each line as a single experiment and each character as a feature. \n",
    "\n",
    "#Load Positive instances \n",
    "training_set=load_dna_dataset(\"dataset/ipdata_tra_t_2018.txt\",label_value=1.0)\n",
    "#Load Negative instances\n",
    "neg_tra=load_dna_dataset(\"dataset/ipdata_tra_f_2018.txt\",label_value=0.0)\n",
    "#Join in a single Dataframe\n",
    "training_set = training_set.union(neg_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#A bit of dataset exploration\n",
    "#\n",
    "\n",
    "#training_set.head()\n",
    "#training_set.describe().show()\n",
    "training_set.printSchema()\n",
    "#training_set.count()\n",
    "#training_set.filter(\"label=1.0\").count()\n",
    "#training_set.filter(\"label=0.0\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Test set\n",
    "test_set=load_dna_dataset(\"dataset/ipdata_test_t_2018.txt\",label_value=1.0)\n",
    "neg_test=load_dna_dataset(\"dataset/ipdata_test_f_2018.txt\",label_value=0.0)\n",
    "test_set = test_set.union(neg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print test_Set schema\n",
    "test_set.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark-ML algorithms requires a single vector containing each features\n",
    "#Assemble vector of features\n",
    "assembler = VectorAssembler(inputCols=training_set.columns[0:len(training_set.columns)-1],outputCol=\"features\")\n",
    "training=assembler.transform(training_set).select(\"label\",\"features\")\n",
    "#Note: we are unsing the same vector assembler instantiated for trainig set.\n",
    "test=assembler.transform(test_set).select(\"label\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assembled Data exploration\n",
    "#training.printSchema()\n",
    "#training.show()\n",
    "training.head()\n",
    "#training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cache data in memory.RDD (or Dataframe) are never persisted automatically by Spark.\n",
    "#Here we use the cache because the same dataset is used for several training tasks\n",
    "training.cache()\n",
    "test.cache()\n",
    "print(\"Training Tot instances: %s\" %training.count())\n",
    "print(\"Test Tot instances: %s\" %test.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#Decision TREE CLASSIFIER\n",
    "#\n",
    "alg_label=\"DECISION TREE CLASSIFIER\"\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\",maxDepth=4)\n",
    "dt_fitted = dt.fit(training)\n",
    "\n",
    "print(\"%s Training time: %5.2f seconds ---\" % (alg_label,(time.time() - start_time)))\n",
    "start_time = time.time()\n",
    "\n",
    "dt_predictions_and_labels = dt_fitted.transform(test)\n",
    "\n",
    "print(\"%s Test: %5.2f seconds ---\" % (alg_label,(time.time() - start_time)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(dt_predictions_and_labels,print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A bit of exploration\n",
    "#print(dt_fitted.featureImportances)\n",
    "print(dt_fitted.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ramdom Forest Classifier TRAINING\n",
    "alg_label=\"RANDOM FOREST CLASSIFIER\"\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",numTrees=100, maxDepth=15)\n",
    "rf_fitted = rf.fit(training)\n",
    "print(\"%s Training time: %5.2f seconds ---\" % (alg_label,(time.time() - start_time)))\n",
    "start_time = time.time()\n",
    "rf_predictions_and_labels = rf_fitted.transform(test)\n",
    "print(\"%s Test: %5.2f seconds ---\" % (alg_label,(time.time() - start_time)))\n",
    "#Evaluation    \n",
    "confusion_matrix(rf_predictions_and_labels,print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear SVM TRAINING\n",
    "alg_label=\"LINEAR SUPPORT VECTOR MACHINE\"\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "start_time = time.time()\n",
    "lsvc = LinearSVC()\n",
    "lsvc_fitted = lsvc.fit(training)\n",
    "print(\"%s Training time: %5.2f seconds ---\" % (alg_label,(time.time() - start_time)))\n",
    "start_time = time.time()\n",
    "lsvc_predictions_and_labels = lsvc_fitted.transform(test)\n",
    "print(\"%s Test: %5.2f seconds ---\" % (alg_label,(time.time() - start_time)))\n",
    "#Evaluation    \n",
    "confusion_matrix(lsvc_predictions_and_labels,print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "alg_label=\"Naive Bayes\"\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "start_time = time.time()\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "nb_fitted = nb.fit(training)\n",
    "print(\"%s Training time: %5.2f seconds ---\" % (alg_label,(time.time() - start_time)))\n",
    "start_time = time.time()\n",
    "nb_predictions_and_labels = nb_fitted.transform(test)\n",
    "print(\"%s Test: %5.2f seconds ---\" % (alg_label,(time.time() - start_time)))\n",
    "#Evaluation    \n",
    "confusion_matrix(nb_predictions_and_labels,print_summary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multilayer Perceptron Classifier\n",
    "alg_label=\"MLP Classifier\"\n",
    "\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "start_time = time.time()\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 240 (Features)\n",
    "# and output of size 2 (classes)\n",
    "layers = [240, 40, 60, 2]\n",
    "\n",
    "# create the trainer and set parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# train the model\n",
    "model = trainer.fit(training)\n",
    "print(\"%s Training time: %5.2f seconds ---\" % (alg_label,(time.time() - start_time)))\n",
    "# compute accuracy on the test set\n",
    "start_time = time.time()\n",
    "nb_predictions_and_labels = model.transform(test)\n",
    "print(\"%s Test: %5.2f seconds ---\" % (alg_label,(time.time() - start_time)))\n",
    "\n",
    "#Print Confusion Matrix\n",
    "confusion_matrix(nb_predictions_and_labels,print_summary=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
